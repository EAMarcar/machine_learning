---
title: "Machine Learning Class Assignment"
author: "Edmund A. Marcarelli"
date: "January 27, 2016"
output:
  html_document:
    keep_md: yes
---

### Executive Summary

The goal of this project is to predict the manner in which an exercise was done. The outcome  variable in the data set is *classe*.  This analysis examines the data and chooses variables to predict with. This report describes how I built the model, how I used cross validation, and what I think the expected out-of-sample error is. I describe what choices were made in pre-processing the data and how the model was chosen. 


```{r prepare, echo=FALSE, warning=FALSE, message=FALSE}

library(caret, quietly = TRUE, warn.conflicts = FALSE)
library(plyr, quietly = TRUE, warn.conflicts = FALSE)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)
library(ROCR, quietly = TRUE, warn.conflicts = FALSE)

## read in training file

train <- read.csv("pml-training.csv", stringsAsFactors=FALSE)

## Partition into test and training sata sets
set.seed(1)
curl <- createDataPartition(y = train$classe, p=0.75, list = FALSE)
trn <- train[curl,]
tst <- train[-curl,]


```


### Preliminary data analysis and data tidying

```{r tidy, echo=FALSE}

## Tidy up data set

## Find near zero variables
nsv <- nearZeroVar(trn,saveMetrics=TRUE)

## get column numbers on non-zero variables
nsv_num <- which(nsv$nzv == FALSE)
## select those columns
trn_T <- select(trn, nsv_num)

## Find variables with lots of missing values
nmissing <- function(x) sum(is.na(x))
miss <- colwise(nmissing)(trn_T)
miss <- which(miss[1,] > 10000  )
## Drop those variables and others not needed
trn_T1 <- select(trn_T,-1,-2,-3,-4,-5, -miss)
num_obs <- dim(trn_T1)[1]
num_vars <- dim(trn_T1)[2]

```
The data set provided for the project involves Human Activity Recognition described here: http://groupware.les.inf.puc-rio.br/har.  It has **`r nrow(train)`** observations and **`r ncol(train)`** variables.

The first step of this analysis is to split the data into training and test sets. Next examine the data set and do some initial tidying it up.  I looked for near-zero variables and variables with significant missing values (more than 10,000 observations).  All of these variables were dropped.  This left a data set with **`r num_obs`** observations and **`r num_vars`** variables. I also dropped variables that won't be used in the analysis: *user_name, timestamps and new_window*.

Given the relatively large number of potential predictors some further analysis of the data is required before selecting features for the model.

### Data analysis and model selection

```{r analyze, echo=FALSE}

## create binary outcome variable
trn_T1$A_curl <- as.factor(ifelse(trn$classe == "A", 1,0))

## The following identifies instruments with major impact on A/Not A classification
## separate A_curl and non-A_curl observations to collect means
trn_A1 <- filter(trn_T1, A_curl == 1)
trn_A0 <- filter(trn_T1, A_curl == 0)

## calculate means for each group 
A0_mn <- numcolwise(mean)(trn_A0, na.rm = TRUE)
A1_mn <- numcolwise(mean)(trn_A1, na.rm = TRUE)
diff_mn = A1_mn - A0_mn
## non-numeris columns are dropped by r 
diff_abs <- as.numeric(abs(diff_mn[1,]))
diff_med <- median(diff_abs)

## Parse training set to select variables above a certain level of difference (for now)
## choose cutoff for variable selection

cutoff <- quantile(abs(diff_mn))[2]  ##  2 = 25%, 3 = 50%, 4 = 75% percentile and above

idx <- length(diff_mn[2,])
pred_list <- 0
## pred_list[1] <- "A_curl" 
j <- 1
for (i in 1:idx) {
      
      if (abs(diff_mn[,i]) >= cutoff) { 
            pred_list[j] <- colnames(diff_mn[i])
      j <- j+1
      } 
}

pred_list[j] <- "A_curl"

```
The variable *classe* indicates how an exercise was done.  "A" indicating a correct execution and all other values (B through E) indicating execution with various mistakes.  To help select predictors, I created a new outcome variable, *A_curl*, which is 1 for correct execution and 0 for incorrect.  I used this variable to do some of my preliminary analysis.  I separated the 1 and zeros and calculated means for each variable by group.  I then took the absolute value of the differences.  I set up a process that let me choose from these variables above a certain threshold in the differences. I selected all variables with a difference in means that put them at 75%  or above in the list of variables, then 50%, then 25% and fit each group to a *glm* model.  As expected each model gave progressively better results for accuracy, sensitivity and specificity as the number of variables increased.

```{r glmmod, echo=FALSE, warning = FALSE}

## Use list from above to select predictors (after converting to column numbers)
pred_cols <- match(pred_list,table= names(trn_T1))
trn_T2 <- select(trn_T1, pred_cols)
pred_num <- length(pred_list) -1  ## subtract 1 to account for outcome variable

## Rum glm model with list of predictors from above
set.seed(1432)
curl_mod <- train(A_curl ~ ., data = trn_T2, method = "glm", family = "binomial")

## Get predictions and confusion matrix
predictions <- predict(curl_mod)
## fix direction so positive = 1
curl_mod_cm <- confusionMatrix(predictions, trn_T2$A_curl, positive = levels(trn_T2$A_curl)[2]) 

curl_mod_acy <- round(curl_mod_cm$overall[1], 3) * 100

```

This analysis identified **`r pred_num`**  variables in the 25 percentile and above that, using a *glm* model, provided for **`r curl_mod_acy`%** accuracy (see below).  

------

```{r vartable, echo=FALSE, fig.width=7, fig.height=4}


## Build table of variables to display

tab_c1 <- tableGrob(pred_list[1:10], core.just = "left", padding.h = unit(10, "mm"))
tab_c2 <- tableGrob(pred_list[11:20], core.just = "left", padding.h = unit(10, "mm"))
tab_c3 <- tableGrob(pred_list[21:30], core.just = "left", padding.h = unit(10, "mm"))
tab_c4 <- tableGrob(pred_list[31:pred_num], core.just = "left", padding.h = unit(10, "mm"))

grid.arrange(tab_c1, tab_c2, tab_c3, tab_c4, ncol=4,
             main=textGrob("Variables Selected by Comparing Mean Differences", gp=gpar(cex=1.5)))


```

------
Next I reverted to *classe* as the outcome variable and used a *random forest* model with these variables.  This produced an even higher level of accuracy.  The results are described below.

```{r rfmod, echo=FALSE, fig.width=7, fig.height=6, message=FALSE}



## This is the rf version of the previous model
## change outcome variable to classe
pred_list[j] <- "classe"
pred_cols <- match(pred_list,table= names(trn_T1))
trn_T2 <- select(trn_T1, pred_cols)

set.seed(3234)

train_control <- trainControl(method="cv", number=3)

curl_modb <- train(classe ~ ., data = trn_T2,  trControl = train_control, method = "rf", importance = TRUE, verbose = FALSE)


## select top 10 in importance  (based on class A)
pred_list3 <- varImp(curl_modb)$importance
pred_list3$vars <- rownames(pred_list3)
pred_list3 <- arrange(pred_list3, desc(A))
## pred_list3$A <- round(pred_list3$Overall, 2)


## results
predictions <- predict.train(curl_modb, type ="raw")
curl_modb_cm <- confusionMatrix(predictions, trn_T2$classe)  
curl_modb_acy <- round(curl_modb_cm$overall[1], 3) * 100

## draw plot of top 25 predictiors
plot(varImp(curl_modb), main = "Top 20 Predictors For Curl Model", top = 20)

```

The  *random forest* model reaches **`r curl_modb_acy`%** accuracy. The plot above shows the relative importance of the top 20 (of `r pred_num`) variables in the model. It demonstrates that importance trails off significantly. Given this diminishing importance, and to minimize the risk of over-fitting, I decided to use just the top 10 variables in a new model. 

The new *random forest* model produced the Importance Plot below.  For this model I returned to the original training set (before any pre-processing had been done) and constructed a set of code that would be used to pre-process both the training and test sets. 

```{r final, echo=FALSE, fig.width=6, fig.height=4, message = FALSE, warning = FALSE}

## Prepare data and run the final model
## preprocess original training and test data sets using the exact same steps)
## drop all but the predictor and outcome variables

##pre-process 
## take top 10 variables from the earlier model as predictors - get column num for select function                       
pred_cols3 <- head(match(pred_list3$vars,table= names(trn)), 10)
## drop all but outcome and predictors
trnnew <- select(trn, pred_cols3, classe)
tstnew <- select(tst, pred_cols3, classe)
## end pre-proccess


## run the final model against the training set
set.seed(2321)
## run model with cross validation
train_control <- trainControl(method="cv", number=3)

curl_mod3 <- train(classe ~., data=trnnew, trControl=train_control, method="rf", importance = TRUE, verbose=FALSE)

## get results
predictions <- predict.train(curl_mod3, type ="raw")
curl_mod3_cm <- confusionMatrix(predictions, trn$classe)  
curl_mod3_acy <- round(curl_mod3_cm$overall[1], 3) * 100

plot(varImp(curl_mod3), main = "Importance Plot for Final Curl Model - Training")



```

The model was run using K-fold cross-validation (k = 3). Somewhat surprisingly, drooping from 40 variables to 10 had little or no adverse affect on the model's accuracy. This model gives this level of accuracy: **`r curl_mod3_acy`%**.  

```{r confmat, echo=FALSE, fig.width=5, fig.height=4}

cm_table <- curl_mod3_cm$table
cm_tab <- tableGrob(cm_table, padding.h = unit(15, "mm"))

grid.arrange(cm_tab, nrow=2,
             main=textGrob("Confusion Matrix Results: Curl Model - Training Data Set"),
            heights = c(1)) 


```

### Results

The final step was to use the model to predict the test data set. The test data set was pre-processed with the identical code used to pre-process the training set.



Note: *This model run was only run once against the test data set. The code in the .Rmd file is set to eval=FALSE and the rsults are "hard-coded" in the report.*

```{r testmod, echo=FALSE, eval=FALSE, message = FALSE, warning = FALSE}

## show code for test run - executed only once
set.seed(5456)
pred_tst <- predict(curl_mod3, newdata = tstnew, type = "raw")

## get results

curl_mod3_cmT <- confusionMatrix(pred_tst, tstnew$classe)  
curl_mod3_acyT <- round(curl_mod3_cm$overall[1], 3) * 100

cm_table <- curl_mod3_cmT$table

```


```{r harcode, echo = FALSE, fig.width=6, fig.height=4}

## Test data results - hard coded

out_acc <- 99.92
out_err <- round(100 - out_acc, 4) 


```


The model indicates accuracy of **`r out_acc`%** and an expected out-of-sample error rate of **`r out_err`%**.  This is based on the accuracy of the model when it was run against the test data set held out from the original data provided.

> Accuracy: `r out_acc`%

> Expected Out-of-sample Error: `r out_err`%



### Conclusion

The curl exercise identified the correct way to execute the curl and several "mistakes."  This report demonstrates that the manner in which the exercise was executed can be identified reliably using 10 variables from the Human Activity Recognition data set that was the subject of this class project.  


.
.
.


      


##### End of report
