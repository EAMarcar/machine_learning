---
title: "Machine Learning Class Assignment"
author: "Edmund A. Marcarelli"
date: "January 23, 2016"
output:
  html_document:
    keep_md: yes
---

### Executive Summary

The goal of this project is to predict the manner in which an exercise was done. The outcome  variable in the dataset is *classe*.  This analysis examines the data and chooses variables to predict with. This report describes how I built the model, how I used cross validation, and what I think the expected out-of-sample error is. I describe what choices were made in preprocessing the data and how the model was chosen. 


```{r prepare, echo=FALSE, warning=FALSE, message=FALSE}

library(caret, quietly = TRUE, warn.conflicts = FALSE)
library(plyr, quietly = TRUE, warn.conflicts = FALSE)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)
library(ROCR, quietly = TRUE, warn.conflicts = FALSE)

## read in training file

train <- read.csv("pml-training.csv", stringsAsFactors=FALSE)

## Partition into test and training sata sets
set.seed(1)
curl <- createDataPartition(y = train$classe, p=0.75, list = FALSE)
trn <- train[curl,]
tst <- train[-curl,]


```


### Preliminary data analysis and data tidying

```{r tidy, echo=FALSE}

## Tidy up data set

## Find near zero variables
nsv <- nearZeroVar(trn,saveMetrics=TRUE)

## get column numbers on non-zero variables
nsv_num <- which(nsv$nzv == FALSE)
## select those columns
trn_T <- select(trn, nsv_num)

## Find variables with lots of missing values
nmissing <- function(x) sum(is.na(x))
miss <- colwise(nmissing)(trn_T)
miss <- which(miss[1,] > 10000  )
## Drop those variables and others not needed
trn_T1 <- select(trn_T,-1,-2,-3,-4,-5, -miss)
num_obs <- dim(trn_T1)[1]
num_vars <- dim(trn_T1)[2]

```
The data set provided for the project involves Human Acttivity Recognition described here: http://groupware.les.inf.puc-rio.br/har.  It has **`r nrow(train)`** observations and **`r ncol(train)`** variables.

The first step of this analysis is to split the data into training and test sets. Next examine the data set and do some intial tidying it up.  I looked for near-zero variables and variables with significant missing values (more than 10,000 observations).  All of these variables were dropped.  This left a data set with **`r num_obs`** observations and **`r num_vars`** variables. I also dropped variables that won't be used in the analysis: user name and timestamps.

Given the relatively large number of potential predictors some further analysis of the data is required before selecting features for the model.

### Data analysis and model selection

```{r analyze, echo=FALSE}

## create binary outcome variable
trn_T1$A_curl <- as.factor(ifelse(trn$classe == "A", 1,0))

## The following identifies instruments with major impact on A/Not A classification
## separate A_curl and non-A_curl observations to collect means
trn_A1 <- filter(trn_T1, A_curl == 1)
trn_A0 <- filter(trn_T1, A_curl == 0)

## calculate means for each group 
A0_mn <- numcolwise(mean)(trn_A0, na.rm = TRUE)
A1_mn <- numcolwise(mean)(trn_A1, na.rm = TRUE)
diff_mn = A1_mn - A0_mn
## non-numeris columns are dropped by r 
diff_abs <- as.numeric(abs(diff_mn[1,]))
diff_med <- median(diff_abs)

## Parse training set to select variables above a certain level of difference (for now)
## choose cutoff for variable selection

cutoff <- quantile(abs(diff_mn))[2]  ##  2 = 25%, 3 = 50%, 4 = 75% percentile and above

idx <- length(diff_mn[2,])
pred_list <- 0
## pred_list[1] <- "A_curl" 
j <- 1
for (i in 1:idx) {
      
      if (abs(diff_mn[,i]) >= cutoff) { 
            pred_list[j] <- colnames(diff_mn[i])
      j <- j+1
      } 
}

pred_list[j] <- "A_curl"

```
The variable *classe* indicates how an exercise was done.  "A" indicating a correct execution and all other values (B through E) indicating execution with various mistakes.  I created a new outcome variable, *A_curl*, which is 1 for correct execution and 0 for incorrect.  I used this variable to do some of my analysis.  I separated the 1 and zeros and calculated means for each variavble by group.  I then took the absolute value of the differences.  I set up a process that let me choose from these variables above a certain threshold in the differences.  Using a glm model I selected all variables with a difference in means that put them at 75%  or above in the list of variables, then 50%, then 25%.  As expected these gave progressively better results for accuracy, sensitivity and specificity as the number of variables increased.

```{r glmmod, echo=FALSE, warning = FALSE}

## Use list from above to select predictors (after converting to column numbers)
pred_cols <- match(pred_list,table= names(trn_T1))
trn_T2 <- select(trn_T1, pred_cols)
pred_num <- length(pred_list) -1  ## subtract 1 to account for outcome variable

## Rum glm model with list of predictors from above
set.seed(2)
curl_mod <- train(A_curl ~ ., data = trn_T2, method = "glm", family = "binomial")

## Get predictions and confusion matrix
predictions <- predict(curl_mod)
## fix direction so positive = 1
curl_mod_cm <- confusionMatrix(predictions, trn_T2$A_curl, positive = levels(trn_T2$A_curl)[2]) 

curl_mod_acy <- round(curl_mod_cm$overall[1], 3) * 100

```

This analysis identified **`r pred_num`**  variables in the 25 percentile and above that, using a *glm* model, provided for **`r curl_mod_acy`%** accuracy (see below).  

------

```{r vartable, echo=FALSE, fig.width=7, fig.height=4}


## Build table of variables to display

tab_c1 <- tableGrob(pred_list[1:10], core.just = "left", padding.h = unit(10, "mm"))
tab_c2 <- tableGrob(pred_list[11:20], core.just = "left", padding.h = unit(10, "mm"))
tab_c3 <- tableGrob(pred_list[21:30], core.just = "left", padding.h = unit(10, "mm"))
tab_c4 <- tableGrob(pred_list[31:40], core.just = "left", padding.h = unit(10, "mm"))

grid.arrange(tab_c1, tab_c2, tab_c3, tab_c4, ncol=4,
             main=textGrob("Variables Selected by Comparing Mean Differences", gp=gpar(cex=1.5)))


```

------
Using a *gbm* model with these variables, I was able to substantially increase accuracy using the same set of variables.  The results are described below.

```{r gbmmod, echo=FALSE, fig.width=6, fig.height=4, message=FALSE}



## This is the gbm version of the previous model
set.seed(3)
curl_modb <- train(A_curl ~ ., data = trn_T2, preProcess = c("center", "scale"), method = "gbm", verbose=FALSE)

## select top 10 in importance

pred_list3 <- varImp(curl_modb)$importance
pred_list3$vars <- rownames(pred_list3)
pred_list3 <- arrange(pred_list3, desc(Overall))
pred_list3$Overall <- round(pred_list3$Overall, 2)


## confusion matrx
predictions <- predict(curl_modb)
curl_modb_cm <- confusionMatrix(predictions, trn_T2$A_curl, positive = levels(trn_T2$A_curl)[2])  
curl_modb_acy <- round(curl_modb_cm$overall[1], 3) * 100

## draw plot of top 25 predictiors
plot(varImp(curl_modb), main = "Top 25 Predictors For Curl Model", top = 25)

```

The  *gbm* model reaches **`r curl_modb_acy`%** accuracy. The plot above shows the relative importance of the top 25 (of 40) variables in the model. It demonstrates that importance trails off significantly. Given this diminishing importance, and to minimize the risk of overfitting, I decided to use just the top 10 variables in a new gbm model. 

The new *gbm* model produced the Importance Plot below.  For this model I returned to the original training set (before any pre-processing had been done) and constructed a set of code that would be used to pre-process both the training and test sets. 

```{r final, echo=FALSE, fig.width=6, fig.height=4, message = FALSE, warning = FALSE}

## Prepare data and run the final model
## preprocess original training date set (exact same steps will be used on test data set)
## two simple steps:    1) create A_curl, two-factor outcome variable
##                      2) drop all butthe predictor and outcome variables

trn$A_curl <- as.factor(ifelse(trn$classe == "A", 1,0))

## take top 10 variables from the earlier model as predictors - get column num for select function                       
pred_cols3 <- head(match(pred_list3$vars,table= names(trn)), 10)
## drop all but outcome and predictors
trn <- select(trn, pred_cols3, A_curl )

## run the final model against the training set
set.seed(4)
##curl_mod3 <- train(A_curl ~ ., data = trn, preProcess = c("center", "scale"), method = "gbm", verbose=FALSE)

## run model with cross validation
# define training control
train_control <- trainControl(method="cv", number=3)
# train the model 
curl_mod3 <- train(A_curl ~., data=trn, trControl=train_control, method="gbm", verbose=FALSE)

## get results
predictions <- predict(curl_mod3)
curl_mod3_cm <- confusionMatrix(predictions, trn$A_curl, positive = levels(trn$A_curl)[2])  
curl_mod3_acy <- round(curl_mod3_cm$overall[1], 3) * 100

plot(varImp(curl_mod3), main = "Importance Plot for Final Curl Model - Training")

```

The model was run using K-fold cross-validation (k = 3). Somewhat surprisingly, droping from 40 variables to 10 had little or no adverse affect on the model's accuracy. This model gives this level of accuracy: **`r curl_mod3_acy` **.  

### Results

The final step was to use the model to predict the test dataset. The test data set was pre-processed with the identical code used to pre-process the training set.


```{r testrun, echo=FALSE, fig.width=6, fig.height=4, message = FALSE, warning = FALSE}

## preprocess exactly as we did the training set
## two simple steps:    1) create A_curl, two-factor outcome variable
##                      2) drop all butthe predictor and outcome variables

tst$A_curl <- as.factor(ifelse(tst$classe == "A", 1,0))

## take top 10 variables from the earlier model as predictors - get column num for select function                       
pred_cols3 <- head(match(pred_list3$vars,table= names(tst)), 10)
## drop all but outcome and predictors
tst <- select(tst, pred_cols3, A_curl )

```

Note: *This model run was only run once against the test data set. The code in the .Rmd file is set to eval=FALSE and the rsults are "hard-coded" in the report.*

```{r testmod, echo=FALSE, eval=FALSE, message = FALSE, warning = FALSE}

## show code for test run - executed only once

## pred_tst <- predict(curl_mod3, newdata = tst)
## get results
## curl_mod4_cm <- confusionMatrix(pred_tst, tst$A_curl, positive = levels(tst$A_curl)[2])  
## curl_mod4_acy <- round(curl_mod4_cm$overall[1], 3) * 100

```

The final jeopardy answer is: ** curl_mod4_acy **

### Conclusion

### Appendix
